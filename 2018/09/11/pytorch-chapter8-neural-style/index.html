<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="neural style transfer," />










<meta name="description" content="1 前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：(https:&#x2F;&#x2F;github.com&#x2F;anishathalye&#x2F;neural-style)">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch chapter8 neural style">
<meta property="og:url" content="http://example.com/2018/09/11/pytorch-chapter8-neural-style/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1 前言本文主要是针对陈云的PyTorch入门与实践的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。这是我的代码大神链接：(https:&#x2F;&#x2F;github.com&#x2F;anishathalye&#x2F;neural-style)">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/pictures/neural-style/pic1.png">
<meta property="article:published_time" content="2018-09-11T03:19:25.000Z">
<meta property="article:modified_time" content="2019-01-14T03:08:44.031Z">
<meta property="article:author" content="TianJiajie">
<meta property="article:tag" content="neural style transfer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/pictures/neural-style/pic1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2018/09/11/pytorch-chapter8-neural-style/"/>





  <title>pytorch chapter8 neural style | Hexo</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-something">
          <a href="/something" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            something
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2018/09/11/pytorch-chapter8-neural-style/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">pytorch chapter8 neural style</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-11T11:19:25+08:00">
                2018-09-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>本文主要是针对<a target="_blank" rel="noopener" href="https://github.com/chenyuntc/pytorch-book">陈云的PyTorch入门与实践</a>的第八章的内容进行复现，准确地说，是看着他写的代码，自己再实现一遍，所以更多地是在讲解实现过程中遇到的问题或者看到的好的方法，而不是针对论文的原理的进行讲解。对于原理，也只是会一笔带过。原理篇暂时不准备留坑，因为原理是个玄学。<br>这是我的<a target="_blank" rel="noopener" href="https://github.com/TJJTJJTJJ/pytorch__learn">代码</a><br>大神链接：(<a target="_blank" rel="noopener" href="https://github.com/anishathalye/neural-style">https://github.com/anishathalye/neural-style</a>)</p>
<hr>
<span id="more"></span>
<h1 id="2-问题及其解决"><a href="#2-问题及其解决" class="headerlink" title="2 问题及其解决"></a>2 问题及其解决</h1><p>我在第六章和第七章的时候还是基于pytorch 0.4.0，而第八章的时候我开始基于pytorch 0.4.1，所以以下的内容介绍都是基于0.4.1</p>
<h2 id="2-1-文件组织形式"><a href="#2-1-文件组织形式" class="headerlink" title="2.1 文件组织形式"></a>2.1 文件组织形式</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">├─checkpoints/</span><br><span class="line">├─content_img/</span><br><span class="line">│  ├─input.jpg</span><br><span class="line">│  ├─output.jpg</span><br><span class="line">│  └─style.jpg</span><br><span class="line">├─data/</span><br><span class="line">│  ├─coco/a.jpg</span><br><span class="line"></span><br><span class="line">├─dataset/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─dataset.py</span><br><span class="line">├─models/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─PackedVGG.py</span><br><span class="line">│  └─transformer_net.py</span><br><span class="line">└─utils/</span><br><span class="line">│  ├─__init__.py</span><br><span class="line">│  └─utils.py</span><br><span class="line">│  └─visualize.py</span><br><span class="line">├─config.py</span><br><span class="line">└─main.py</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中，上半部分是对数据和模型的保存组织形式，我们只需要能对应起来即可，其中，checkpoints是为了保存模型，content_img中的style.jpg是训练时候的风格图片，input.jpg是测试的输入，output.jpg是测试的输出，data中的数据是训练数据，主要是因为这个训练数据太整齐，是用ImageFolder读取的，为了避免麻烦，也为了在测试的时候方便观察图片，所以style.jpg我们暂时放在了content中。<br>下半部分是重点，我们需要写的代码，每次都是先从dataset.py和models开始写起，然后导入visualize.py，这个文件基本不会发生改变，然后同时写main.py和config.py，边写边扩展utils中的其他文件，例如main中用到的函数等等。</p>
<h2 id="2-2-models"><a href="#2-2-models" class="headerlink" title="2.2 models"></a>2.2 models</h2><h3 id="PackedVGG-py"><a href="#PackedVGG-py" class="headerlink" title="PackedVGG.py"></a>PackedVGG.py</h3><p>这里我们主要是取已有的网络，得到中间层的输出<br><strong>models.named_parameters()</strong>:返回的是一个生成器，每次返回一个参数的关键字和值<br><strong>models.state_dict()</strong>:返回的是一个字典，记录了参数的关键字和值<br><strong>models.parameters()</strong>:返回的是变量，没有名字，可以在requires_grad中用到<br>models.features返回的是相对应的模型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: <span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: models = vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: model = models.features[:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: model</span><br><span class="line">Out[<span class="number">10</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: models.parameters()</span><br><span class="line">Out[<span class="number">11</span>]: &lt;generator <span class="built_in">object</span> Module.parameters at <span class="number">0x7f8fad26b3b8</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: models.named_parameters()</span><br><span class="line">Out[<span class="number">12</span>]: &lt;generator <span class="built_in">object</span> Module.named_parameters at <span class="number">0x7f8f29e99d58</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: model.named_parameters()</span><br><span class="line">Out[<span class="number">13</span>]: &lt;generator <span class="built_in">object</span> Module.named_parameters at <span class="number">0x7f8fad26b2b0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: model.parameters()</span><br><span class="line">Out[<span class="number">14</span>]: &lt;generator <span class="built_in">object</span> Module.parameters at <span class="number">0x7f8fad26b4c0</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: model.state_dict()</span><br><span class="line">Out[<span class="number">15</span>]: </span><br><span class="line">OrderedDict([(<span class="string">&#x27;0.weight&#x27;</span>, tensor([[[[-<span class="number">0.5537</span>,  <span class="number">0.1427</span>,  <span class="number">0.5290</span>],</span><br><span class="line">                        [-<span class="number">0.5831</span>,  <span class="number">0.3566</span>,  <span class="number">0.7657</span>],</span><br><span class="line">                        [-<span class="number">0.6902</span>, -<span class="number">0.0480</span>,  <span class="number">0.4841</span>]],</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line">models = vgg16(pretarined = <span class="literal">True</span>)</span><br><span class="line">In [<span class="number">19</span>]: models</span><br><span class="line">Out[<span class="number">19</span>]: </span><br><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">1000</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: models.features</span><br><span class="line">Out[<span class="number">20</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">3</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">6</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">8</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">11</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">13</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">15</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">18</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">20</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">22</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">25</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">27</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">29</span>): ReLU(inplace)</span><br><span class="line">  (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: models.features[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">21</span>]: ReLU(inplace)</span><br><span class="line"><span class="comment"># list</span></span><br><span class="line">In [<span class="number">27</span>]: models4 = models2[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: models4</span><br><span class="line">Out[<span class="number">28</span>]: </span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: models4list</span><br><span class="line">Out[<span class="number">32</span>]: </span><br><span class="line">[Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line"> ReLU(inplace)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: models4list[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">36</span>]: ReLU(inplace)</span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: models4list[<span class="number">1</span>].named_parameters</span><br><span class="line">Out[<span class="number">37</span>]: &lt;bound method Module.named_parameters of ReLU(inplace)&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>sequencial是支持索引操作的<br>list(module)会变成一个list，可以通过索引来获取层，注意，nn.ModuleList, nn.Sequential, nn.Conv等都是Module,都可以通过named_parameters来获取参数。<br>为了能够提取出中间层的输出，作者换了一个方法，用的nn.ModuleList,nn.ModuleList和nn.Sequential的区别在此才真正显现，nn.Sequential更有利于直接把输入传给Module，计算是一个整体，写起来更方便，而nn.Modulist则不能直接把输入传给Module，需要用循环传输入，更有利于在层中做一些保留，提取中间层的输出。后面我们会讲到hook。或者说提取中间层的输出我们可以选择在定义网络的forward中进行，另外，就是需要注意的是，这里的输入是一个batch_size大小的矩阵，所以即便像作者这样，用一个列表保存输出，但实际输出的列表中的元素都是(b,n,h,w)大小的。后面我会验证。</p>
<p>提取中间层的输出有两种方法：<br>第二种方法参考链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0a23db1df55a">https://www.jianshu.com/p/0a23db1df55a</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一种方法，这种方法是在前向网络中提取输出，好像也是在反向传播网络中，但这种提取中间层是永久性的，也适合用这些层的做其他运算，这些运算是计算在整体网络框架中的</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> ii, model <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.features):</span><br><span class="line">        x = model(x)</span><br><span class="line">        <span class="keyword">if</span> ii <span class="keyword">in</span> &#123;<span class="number">3</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">22</span>&#125;:</span><br><span class="line">            results.append(x)</span><br><span class="line"></span><br><span class="line">    vgg_outputs = namedtuple(<span class="string">&quot;VggOutputs&quot;</span>, [<span class="string">&#x27;relu1_2&#x27;</span>, <span class="string">&#x27;relu2_2&#x27;</span>, <span class="string">&#x27;relu3_3&#x27;</span>, <span class="string">&#x27;relu4_3&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> vgg_outputs(*results)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二种方法，适合在在不影响整体网络的情况下拿出一个分支进行单独计算，现在还不清楚这样子会不会影响backward，个人感觉会，因为也是相当于一个变量对其进行计算，导数为1。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x= self.model(x)</span><br><span class="line">    self.fea = x</span><br><span class="line">   x = self.main(x)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="transformer-py"><a href="#transformer-py" class="headerlink" title="transformer.py"></a>transformer.py</h3><p>可参考<a target="_blank" rel="noopener" href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py">链接</a></p>
<ol>
<li>padding的操作是边界反射补充</li>
<li>放大方法是双线性插值，而不是ConvTransposed2d，即unsample或者说是interpolate， 但是其中的一个参数align_corners一直<strong>没有理解</strong>，既然是双线性插值，那结果就是固定的，怎么还会因为其他参数发生变化。</li>
<li><p>其中，写的时候必要的时候可以写写子网络<br>这里我对residualblock提出了疑问，事实上left+right后面可以没有relu层，这一点我们可以从以下链接找到说明。<br><a target="_blank" rel="noopener" href="https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py">https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py</a><br><a target="_blank" rel="noopener" href="http://torch.ch/blog/2016/02/04/resnets.html">http://torch.ch/blog/2016/02/04/resnets.html</a></p>
<blockquote>
<p>The above result seems to suggest that it’s important to avoid changing data that passes through identity connections only. We can take this philosophy one step further: should we remove the ReLU layers at the end of each residual block? ReLU layers also perturb data that flows through identity connections, but unlike batch normalization, ReLU’s idempotence means that it doesn’t matter if data passes through one ReLU or thirty ReLUs. When we remove ReLU layers at the end of each building block, we observe a small improvement in test performance compared to the paper’s suggested ReLU placement after the addition. However, the effect is fairly minor. More exploration is needed.</p>
</blockquote>
</li>
<li><p>对于其他的出现的网络架构，其实都是有理可循的，但暂时不是本篇的重点，所以只做一个记录。<a target="_blank" rel="noopener" href="https://distill.pub/2016/deconv-checkerboard/">上卷积</a>简单地看了看这篇论文，unsample要比ConvTransposed2D要好，但是没有看懂。留作后续。  </p>
</li>
</ol>
<hr>
<h3 id="dataset-py-amp-visualize-py"><a href="#dataset-py-amp-visualize-py" class="headerlink" title="dataset.py &amp; visualize.py"></a>dataset.py &amp; visualize.py</h3><p>因为加载数据是用的tv.datasets.ImageFolder，所以dataset.py不需要写，<br>visualize.py是第六章的时候写好的，这里只写几个改进的</p>
<ol>
<li>self.vis = Visdom(env=env,use_incoming_socket=False, **kwargs)，这里的use_incoming_socket是不需要从浏览器接受数据到软件中，如果没有的话会提示 ‘&gt;’ not supported between instances of ‘float’ and ‘NoneType’</li>
<li>在一个函数前提示输入的大小和类型是一件很重要的事情，必要的时候需要输入分布，</li>
<li>这里的plot用了一个很巧的方法，用字典记录不同的点<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.index = &#123;&#125;</span><br><span class="line">x = self.index.get(win,<span class="number">0</span>)</span><br><span class="line">self.index[win] = x+<span class="number">1</span></span><br></pre></td></tr></table></figure>
其他的细节可以看代码中的记录，应该比较清晰了。</li>
</ol>
<hr>
<h3 id="main-py-amp-utils-py-amp-config-py"><a href="#main-py-amp-utils-py-amp-config-py" class="headerlink" title="main.py &amp; utils.py &amp;  config.py"></a>main.py &amp; utils.py &amp;  config.py</h3><p>其中utils主要为main提供一些用到的函数，config提供参数，<br>main作为主函数，里面主要就是train(),val(),test(),help(),下面记录一些写main函数的一些疑问。</p>
<h4 id="cuda"><a href="#cuda" class="headerlink" title="cuda"></a>cuda</h4><p>这里写几种怎么从cpu到gpu的方法以及应用场景。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 第一种</span><br><span class="line">device = t.device(&#x27;cuda&#x27;) if opt.use_gpu else t.device(&#x27;cpu&#x27;)</span><br><span class="line">models.to(device)</span><br><span class="line">tensor = tensor.to(device)</span><br><span class="line">此时使用默认的cuda，一般是cuda:0，适用于全局</span><br><span class="line"></span><br><span class="line"># 第二种</span><br><span class="line">torch.cuda.current_device() # 查询当前GPU</span><br><span class="line">torch.cuda.set_device(1)</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;)</span><br><span class="line">models.to(device)</span><br><span class="line">此时用的是cuda:1，使用于全局</span><br><span class="line"></span><br><span class="line">#第三种</span><br><span class="line">#上下文管理器</span><br><span class="line">with torch.cuda.device(1):</span><br><span class="line">    models.to(device)</span><br><span class="line">#第四种</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]=&quot;2&quot;</span><br><span class="line">没用过</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h3 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a><a href="">tqdm</a></h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/langb2014/article/details/54798823?locationnum=8&amp;fps=1">https://blog.csdn.net/langb2014/article/details/54798823?locationnum=8&amp;fps=1</a><br>进度条，但是只在jupyter和终端中用的时候效果很明显，在代码中用的效果没有那么好，tqdm试了试，用在enumerate()中时，需要写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">elements = (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(elements)):</span><br><span class="line">    <span class="built_in">print</span>(count, i)</span><br><span class="line"><span class="comment"># two arguments</span></span><br><span class="line"><span class="keyword">for</span> count, ele <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(elements), total=<span class="built_in">len</span>(train_ids), leave=<span class="literal">False</span>):</span><br><span class="line">    <span class="built_in">print</span>(count, i)</span><br></pre></td></tr></table></figure><br>包括zip也是一样，因为他们返回的是一个生成器，并不知道长度。</p>
<p>tqdm的进一步用法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">pb = tqdm(total=<span class="built_in">len</span>(idxs))</span><br><span class="line">pb.set_description(<span class="string">&#x27;&#123;&#125;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(phase, domain))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">	pb.update(<span class="number">1</span>)</span><br><span class="line">pb.close() </span><br></pre></td></tr></table></figure>
<h3 id="反向传播和梯度下降"><a href="#反向传播和梯度下降" class="headerlink" title="反向传播和梯度下降"></a>反向传播和梯度下降</h3><p>参考链接<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16234613/article/details/80025832">https://blog.csdn.net/qq_16234613/article/details/80025832</a><br>这里主要是针对第七章和第八章出现的反向传播和梯度下降出现的问题进行记录。<br>在第七章，是这么实现分别训练的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fake_img =  netg(noises).detach() </span><br><span class="line">fake_output = netd(fake_img)</span><br><span class="line">error_d_fake = criterion(fake_output, fake_labels)</span><br><span class="line">error_d_fake.backward()</span><br><span class="line">optimizer_d.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">optimizer_g.zero_grad()</span><br><span class="line">noises.data.copy_(t.randn(opt.batch_size, opt.nz, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">fake_img = netg(noises)</span><br><span class="line">output = netd(fake_img)</span><br><span class="line">error_g = criterion(output, true_labels)</span><br><span class="line">error_g.backward()</span><br><span class="line">optimizer_g.step()</span><br></pre></td></tr></table></figure><br>y = x.detach()：表示将生成一个新的叶子节点，值与当前节点的值相同，但是y.requires_grad = False, y.grad_fn=None，此时x和y共享内存，对y数据的操作也会影响x，可以理解为冻结了通过y进行反向传播的路。如果在网络的输出detach，即y= models(x).detach()，可以理解成，models只进行前向传播，grad=None。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [17]: a = torch.ones(3,3)</span><br><span class="line"></span><br><span class="line">In [18]: a.requires_grad=True</span><br><span class="line"></span><br><span class="line">In [19]: b = a*2</span><br><span class="line"></span><br><span class="line">In [20]: b.requires_grad</span><br><span class="line">Out[20]: True</span><br><span class="line"></span><br><span class="line">In [21]: b.grad_fn</span><br><span class="line">Out[21]: &lt;MulBackward at 0x7f8fac6e40f0&gt;</span><br><span class="line"></span><br><span class="line">In [22]: c = b.detach()</span><br><span class="line"></span><br><span class="line">In [23]: c.requires_grad</span><br><span class="line">Out[23]: False</span><br><span class="line"></span><br><span class="line">In [24]: print(c.grad_fn)</span><br><span class="line">None</span><br><span class="line">In [25]: c.is_leaf</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">In [2]: a = torch.ones(3,3)</span><br><span class="line">In [14]: b</span><br><span class="line">Out[14]: </span><br><span class="line">tensor([[2., 2., 2.],</span><br><span class="line">        [2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In [15]: c =  b.detach()</span><br><span class="line"></span><br><span class="line">In [16]: c</span><br><span class="line">Out[16]: </span><br><span class="line">tensor([[2., 2., 2.],</span><br><span class="line">        [2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]])</span><br><span class="line"></span><br><span class="line">In [17]: c[0,0]=1</span><br><span class="line"></span><br><span class="line">In [18]: c</span><br><span class="line">Out[18]: </span><br><span class="line">tensor([[1., 2., 2.],</span><br><span class="line">        [2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]])</span><br><span class="line"></span><br><span class="line">In [19]: b</span><br><span class="line">Out[19]: </span><br><span class="line">tensor([[1., 2., 2.],</span><br><span class="line">        [2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]], grad_fn=&lt;MulBackward&gt;)</span><br><span class="line"></span><br><span class="line">In [20]: c.requires_grad</span><br><span class="line">Out[20]: False</span><br><span class="line"></span><br><span class="line">In [21]: b.grad_fn</span><br><span class="line">Out[21]: &lt;MulBackward at 0x7f764429ffd0&gt;</span><br><span class="line"></span><br><span class="line">In [22]: b.grad_fn.next_functions</span><br><span class="line">Out[22]: ((&lt;AccumulateGrad at 0x7f7644428358&gt;, 0),)</span><br><span class="line"></span><br><span class="line">In [23]: a.grad_fn</span><br></pre></td></tr></table></figure><br>在第八章，是这么表示的<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for param in vgg16.parameters():</span><br><span class="line">    param.requires_grad = False</span><br></pre></td></tr></table></figure><br>这种表示可以使得某一个网络不参与梯度下降这个过程，但是对于网络的输入和输出还是支持梯度下降的。<br>requires_grad只是表示当前的变量不再需要梯度下降，<br>综上所述，对于中间变量，需要使用x.detach()，使其变成默认的叶子节点，对于叶子节点，使用x.requires_grad。并且对于中间变量使用requires_grad会报错。</p>
<p>在第八章，还有一种表示方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    features = vgg16(style_img)</span><br><span class="line">    gram_style = [gram_matrix(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> features]</span><br><span class="line"></span><br><span class="line"><span class="meta">@t.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">stylize</span>(<span class="params">**kwargs</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><br>这种方法会使得任何计算得到的结果都是requires_grad = False,暂时不清楚和detach()的区别。也是一种表示只前向传播的方法，不参与反向传播和梯度下降。</p>
<h3 id="train"><a href="#train" class="headerlink" title="train()"></a>train()</h3><p>图片分为两种：风格图片，只需要一张，内容图片，很多，用于训练，这一点没有暂时没有理解为什么这么设置。其中，对输入的图片进行了乘以255，我觉得是因为为了使模型的输出直接就是255，不需要再进行处理，没有验证。<br>ensor.item()<br> tensor.tolist()<br>content_image = tv.datasets.folder.default_loader(opt.content_path)<br>在训练过程中，会发现对于整个训练过程，不仅有神经网络，而且还有自己定义的函数，nn.functional，还有两个损失函数，这是之前没有预料到的。</p>
<h3 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存图片的几种方法，第七章的是 </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">tv.utils.save_image(fix_fake_imgs,<span class="string">&#x27;%s/%s.png&#x27;</span> % (opt.img_save_path, epoch),normalize=<span class="literal">True</span>, <span class="built_in">range</span>=(-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># vis.save竟然没找到  我的神   </span></span><br><span class="line"><span class="comment"># 0-1</span></span><br><span class="line">vis.img(<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line">vis.save([opt.env])</span><br></pre></td></tr></table></figure>
<h3 id="utils-py"><a href="#utils-py" class="headerlink" title="utils.py"></a>utils.py</h3><p>这里的疑问是得到gram矩阵的时候，为什么要除以c*h*w,而不是h*w，虽然源码都是这么写的。</p>
<p>写到这里也还是还要很多疑问，暂时保留。<br>昨天发现训练的过程不对，今天在对比代码的过程中，发现了自己写代码的一些漏洞，主要有</p>
<ol>
<li>命名不规范：表示同一个东西出现了两个命名，导致了自己在写代码的过程中传参出现了问题，或者是一类东西没有一个规则进行命名，导致自己在写代码的过程中用到之前的变量的时候必须返回去去查找这个变量，效率低且容易出错。</li>
<li>对源码的修改不是很恰当，导致在写上卷积层的输出和源码完全不一致，这个是自己之前没有遇到的。</li>
<li>visdom的运用，我用不同的environment导致结果也不一样，default是之前一直用的，这次换成了test1之后显示的结果就对了。这个暂时还不清楚原因，如果是会保留信息的话，但是plot是重新开始画的，等会测试测试vis的问题。是网络的问题。但是vis.save()的介绍是序列化信息，暂时还没有理解。</li>
</ol>
<hr>
<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h1 id="对单张图片进行加载验证"><a href="#对单张图片进行加载验证" class="headerlink" title="对单张图片进行加载验证"></a>对单张图片进行加载验证</h1><p>content_image = tv.datasets.folder.default_loader(opt.content_path)<br>可以理解成Image.open，看源码就可以知道的</p>
<p>贴两个成果图看看效果。<br><img src="/pictures/neural-style/pic1.png" alt="训练过程中的图片"></p>
<h2 id="遗留的问题"><a href="#遗留的问题" class="headerlink" title="遗留的问题"></a>遗留的问题</h2><p>Gram矩阵为什么可以代表图片风格，这里有个解释(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。">https://arxiv.org/pdf/1701.01036.pdf)，还没来得及看。</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/neural-style-transfer/" rel="tag"># neural style transfer</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/06/data/" rel="next" title="data">
                <i class="fa fa-chevron-left"></i> data
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/13/numpy/" rel="prev" title="numpy">
                numpy <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">76</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">73</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">1 前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E9%97%AE%E9%A2%98%E5%8F%8A%E5%85%B6%E8%A7%A3%E5%86%B3"><span class="nav-number">2.</span> <span class="nav-text">2 问题及其解决</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E6%96%87%E4%BB%B6%E7%BB%84%E7%BB%87%E5%BD%A2%E5%BC%8F"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 文件组织形式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-models"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PackedVGG-py"><span class="nav-number">2.2.1.</span> <span class="nav-text">PackedVGG.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer-py"><span class="nav-number">2.2.2.</span> <span class="nav-text">transformer.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dataset-py-amp-visualize-py"><span class="nav-number">2.2.3.</span> <span class="nav-text">dataset.py &amp; visualize.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#main-py-amp-utils-py-amp-config-py"><span class="nav-number">2.2.4.</span> <span class="nav-text">main.py &amp; utils.py &amp;  config.py</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cuda"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">cuda</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tqdm"><span class="nav-number">2.2.5.</span> <span class="nav-text">tqdm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.2.6.</span> <span class="nav-text">反向传播和梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train"><span class="nav-number">2.2.7.</span> <span class="nav-text">train()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87"><span class="nav-number">2.2.8.</span> <span class="nav-text">保存图片</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#utils-py"><span class="nav-number">2.2.9.</span> <span class="nav-text">utils.py</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.3.</span> <span class="nav-text"> </span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%B9%E5%8D%95%E5%BC%A0%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E5%8A%A0%E8%BD%BD%E9%AA%8C%E8%AF%81"><span class="nav-number">3.</span> <span class="nav-text">对单张图片进行加载验证</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%81%97%E7%95%99%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">3.1.</span> <span class="nav-text">遗留的问题</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TianJiajie</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
